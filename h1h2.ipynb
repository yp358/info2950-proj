{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 2950 Final Project - Phase II\n",
    "##### Janice Shen (js3678), Khai Xin Kuan (kk996), Sandy Lin (sl2534), David Park (yp358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_clean_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load all datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_clean_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m final\n",
      "File \u001b[0;32m/opt/anaconda3/envs/info2950/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/info2950/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/info2950/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/info2950/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/info2950/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_clean_data.csv'"
     ]
    }
   ],
   "source": [
    "#load all datasets\n",
    "final = pd.read_csv(\"final_clean_data.csv\")\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation visualization and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the large data frame that contains all relevant information about both the music sentiments and the economic metrics, we made a correlation matrix, examining the correlation of how and if certain economic variables correspond with a certain change in music sentiments. \n",
    "\n",
    "Although we note that there is a strong correlation between US_GDP and Entertainment_GDP (0.993423), US_GDP and year (0.995226), and Entertainment_GDP and Year (0.995377). However, since our research question aims to tackle the relationship between music sentiment variables (valence, danceability) with key economic indicators, we discern these strong correlations to be deemed as insignificant.\n",
    "\n",
    "With that said, the correlation matrix reveals no strong relationship between music sentiment and different economic factors. \n",
    "\n",
    "The strongest relationship is between US GDP and valence with a negative correlation of -0.20. Valence and fedfundrate also have a weak positive relationship of 0.13. Valence and the Unemployment rate have a weak positive relationship of 0.09. \n",
    "\n",
    "The correlation is even worse for danceability, where the correlation between danceability and all other economic metrics (fedfundrate, unemployment rate, and US GDP) are < 0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name newmusicdata does not exist!\nDid you mean \"temp.information_schema.schemata\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatalogException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cleandata\u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT danceability,valence, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m            newmusicdata.Year AS year, fedfundrate, UnemploymentRate, recession, election FROM \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m            newmusicdata INNER JOIN econdata ON \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m            newmusicdata.Year = econdata.Year\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdf()\n\u001b[1;32m      5\u001b[0m cleandata\u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT danceability,valence, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m        cleandata.Year, recession, election,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m        fedfundrate, UnemploymentRate, US_GDP, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m        Entertainment_GDP FROM cleandata INNER JOIN gdp_entgdp_df\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m        ON cleandata.year = gdp_entgdp_df.year\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdf()\n\u001b[1;32m     10\u001b[0m cleandata\n",
      "File \u001b[0;32m/opt/anaconda3/envs/info2950/lib/python3.12/site-packages/duckdb/__init__.py:457\u001b[0m, in \u001b[0;36msql\u001b[0;34m(query, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     conn \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:default:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39msql(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mCatalogException\u001b[0m: Catalog Error: Table with name newmusicdata does not exist!\nDid you mean \"temp.information_schema.schemata\"?"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandata.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 1\n",
    "US gdp and entertainment gdp are the most correlated, therefore we chose to run a regression on this.\n",
    "\n",
    "Hypothesis 1: The average valence of popular songs by year from 1960-2016 in the US has an inverse relationship with the corresponding US Gross Domestic Product (GDP) of the given year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= cleandata[\"US_GDP\"]\n",
    "y= cleandata[\"danceability\"] \n",
    "model = LinearRegression().fit(X, y)\n",
    "print(f\"GDP Coeff: {var_coef:.2f}\")\n",
    "print(f\"intercept: {model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= cleandata[input_vars]\n",
    "train_predictions = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = cleandata[\"danceability\"] - train_predictions\n",
    "residuals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_residual_plot(pred, resid):\n",
    "    sns.scatterplot(x= pred, y=resid, marker=\"o\")\n",
    "    plt.axhline(y=0 ,color=\"black\")\n",
    "    plt.xlabel(\"Predicted Danceability\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.show()\n",
    "generate_residual_plot(train_predictions, residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xmar = cleandata[[\"fedfundrate\", \"US_GDP\", \"recession\", \"fertility\"]]\n",
    "ymar = cleandata[[\"danceability\"]]\n",
    "\n",
    "Xmar = sm.add_constant(Xmar)\n",
    "est = sm.OLS(ymar, Xmar).fit()\n",
    "print('Multivar OLS Regression for Danceability:')\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis two: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = [ \"UnemploymentRate\", \"election\", \\\n",
    "              \"DeathRate\", \"S&P500\"]\n",
    "\n",
    "## A5 part a: your code here\n",
    "\n",
    "X= cleandata[input_vars]\n",
    "y= cleandata[\"valence\"] \n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Given code\n",
    "for var_name, var_coef in zip(input_vars, model.coef_):\n",
    "    # A5 part b: your code here\n",
    "    print(f\"{var_name}: {var_coef:.2f}\")\n",
    "\n",
    "# A5 part c: your code here\n",
    "print(f\"intercept: {model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= cleandata[input_vars]\n",
    "train_predictions = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = cleandata[\"valence\"] - train_predictions\n",
    "residuals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_residual_plot(pred, resid):\n",
    "    sns.scatterplot(x= pred, y=resid, marker=\"o\")\n",
    "    plt.axhline(y=0 ,color=\"black\")\n",
    "    plt.xlabel(\"Predicted Danceability\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.show()\n",
    "generate_residual_plot(train_predictions, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmar = cleandata[[ \"UnemploymentRate\", \"election\", \\\n",
    "              \"DeathRate\", \"S&P500\"]]\n",
    "ymar = cleandata[[\"valence\"]]\n",
    "\n",
    "Xmar = sm.add_constant(Xmar)\n",
    "est = sm.OLS(ymar, Xmar).fit()\n",
    "print('Multivar OLS Regression for Valence:')\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multivariable - which variable chose and why. single variable regresion for each and ensured each of them are random residual as possible and use them (explain limitations)\n",
    "single variable regression - indepth about transformation, plot them, see if can improve the model, log, square. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Within Music Sentiment Variables: Danceability vs Valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize how valence trends with danceability over time because we want to observe any peculiarities that may have skewed the correlation result. As observed in the correlation matrix above, we saw that the correlation between valence and danceability is 0.41, which was surprising as we expected it to be higher. Therefore, we plotted it to if the direction that both variables moved were always different or if there was something else going on. From the graph, we can see that the overall trend between danceability and valence is mostly similar, except for the period from 2012 onwards, where danceability increases while valence decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"valence\", y=\"danceability\", data= cleandata, marker=\"o\")\n",
    "plt.title('Danceability agaisnt Valence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time series of valence and danceability across the years\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "sns.lineplot(x='year', y='danceability', data=cleandata,\\\n",
    "    ax=ax1, color='blue', label='Danceability')\n",
    "ax1.set_ylabel('Danceability', color='blue') \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "sns.lineplot(x='year', y='valence', data=cleandata, \\\n",
    "    ax=ax2, color='red', label='Valence')\n",
    "ax2.set_ylabel('Valence', color='red') \n",
    "\n",
    "ax1.set_title('Danceability and Valence Over the Years')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "# fix legends: Add them manually by using the correct handles\n",
    "lines1, _ = ax1.get_legend_handles_labels()\n",
    "lines2, _ = ax2.get_legend_handles_labels() \n",
    "\n",
    "# add both legends to ax1, ensuring the labels are correct\n",
    "ax1.legend(lines1, ['Danceability'], loc='upper left')\n",
    "ax2.legend(lines2, ['Valence'], loc='upper right')\n",
    "\n",
    "# reseting the x-axis ticks to be per year\n",
    "years = np.arange(int(cleandata['year'].min()),\\\n",
    "    int(cleandata['year'].max()) + 1, 1)\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting To Visualize the Trends Between Different Music Variables Vs Different Economic Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Sentiment vs. Federal Fund Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we start our exploration with music variables with economic variables, we first plot the two music sentiment variables with the federal fund rate to visualize why the federal fund rate vs. valence has the strongest correlation (albeit being a weak correlation) amongst economic variables, while having only a -0.01 correlation for federal fund rate vs. danceability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first plotted fedfundrate against valence across the years. Interestingly, we do observe fedfundrate and valence going in similar directions. However, the spikes for fedfundrate are a lot more extreme, with extreme ups and extreme downs. On the other hand, valence progresses a lot more steadily, with a steady decline over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time series of valence and fedfundrate across the years\n",
    "fig, ax1 = plt.subplots()\n",
    "sns.lineplot(x='year', y='fedfundrate', data=cleandata, \\\n",
    "    ax=ax1, color='blue', label='fedfundrate')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='year', y='valence', data=cleandata, \\\n",
    "    ax=ax2, color='red', label='valence')\n",
    "\n",
    "ax1.set_title('Fed Fund Rate and Valence Over the Years')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "lines1, _ = ax1.get_legend_handles_labels() \n",
    "lines2, _ = ax2.get_legend_handles_labels()  \n",
    "\n",
    "ax1.legend(lines1, ['Fed Fund Rate'], loc='upper left')\n",
    "ax2.legend(lines2, ['Valence'], loc='upper right')\n",
    "\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plotted fedfundrate against danceability. We don't observe any trend between the two variables. While Danceability remained stable, fedfundrate had a large spike, which later plummeted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time series of danceability and fedfundrate across the years\n",
    "fig, ax1 = plt.subplots()\n",
    "sns.lineplot(x='year', y='fedfundrate', data=cleandata, \\\n",
    "    ax=ax1, color='blue', label='fedfundrate')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='year', y='danceability', data=cleandata,\\\n",
    "    ax=ax2, color='red', label='danceability')\n",
    "\n",
    "\n",
    "ax1.set_title('Fed Fund Rate and Valence Over the Years')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "lines1, _ = ax1.get_legend_handles_labels()  # Get handles for ax1\n",
    "lines2, _ = ax2.get_legend_handles_labels()  # Get handles for ax2\n",
    "\n",
    "ax1.legend(lines1, ['Fed Fund Rate'], loc='upper left')\n",
    "ax2.legend(lines2, ['Danceability'], loc='upper right')\n",
    "\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Variables vs. Unemployment Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also examine the relationship between music variables vs. unemployment rate to explain why we see similar correlation patterns: why is unemployment rate with valence slightly positive (0.09) while being extremely weakly negative (-0.01) with danceability? Also, we aim to see if we could observe any patterns despite the weak correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we looked at the relationship between the unemployment rate and valence. The plot doesn't seem to indicate any trends between unemployment Rate and valence Throughout the lineplot, valence seems to be steadily going down while the unemployment rate seems to be spiking up and down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time series of valence and unemploymentrate across the years\n",
    "fig, ax1 = plt.subplots()\n",
    "sns.lineplot(x='year', y='UnemploymentRate', data=cleandata, ax=ax1,\\\n",
    "    color='blue', label='UnemploymentRate')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='year', y='valence', data=cleandata, ax=ax2, \\\n",
    "    color='red', label='valence')\n",
    "\n",
    "ax1.set_title('Unemployment Rate and Valence Over the Years')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "lines1, _ = ax1.get_legend_handles_labels()\n",
    "lines2, _ = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1, ['Unemployment Rate'], loc='upper left')\n",
    "ax2.legend(lines2, ['Valence'], loc='upper right')\n",
    "\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plotted danceability with unemployment. Again, given the low correlation in the correlation matrix, these two factors interestingly look like they have a relationship. While unemployment spiked more severely over the years, with ups and downs, music danceability follows a similar trend but remains more steady around a small range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time series of danceability and unemploymentrate across the years\n",
    "fig, ax1 = plt.subplots()\n",
    "sns.lineplot(x='year', y='UnemploymentRate', data=cleandata, ax=ax1, \\\n",
    "    color='blue', label='UnemploymentRate')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='year', y='danceability', data=cleandata, ax=ax2, \\\n",
    "    color='red', label='danceability')\n",
    "\n",
    "ax1.set_title('Unemployment Rate and Danceability Over the Years')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "lines1, _ = ax1.get_legend_handles_labels()\n",
    "lines2, _ = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1, ['Unemployment Rate'], loc='upper left')\n",
    "ax2.legend(lines2, ['Danceability'], loc='upper right')\n",
    "\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Data vs. US GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plotted danceability with US GDP. There appears to be no correlation as danceability remains stable, while US GDP increased throughout the year, danceability remains similar, oscillating slightly in the center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "sns.lineplot(x='year', y='US_GDP', data=cleandata, ax=ax1, \n",
    "             color='blue', label='US GDP')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='year', y='danceability', data=cleandata, ax=ax2, \n",
    "             color='red', label='danceability')\n",
    "plt.title('US GDP and Danceability Over the Years')\n",
    "\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plotted valence with US GDP. There appears to be negative correlation. While valence steadily and slowly decreased over time, US GDP increased linearly over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "sns.lineplot(x='year', y='US_GDP', data=cleandata, ax=ax1, \\\n",
    "             color='blue', label='US GDP')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='year', y='valence', data=cleandata, \n",
    "             ax=ax2, color='red', label='valence')\n",
    "plt.title('US GDP and Valence Over the Years')\n",
    "\n",
    "ax1.set_xticks(years)\n",
    "ax1.set_xticklabels(years, rotation=45) \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot to Analyze Data Points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scatterplot, we inspect why dancebility and valence has such weak correlation with fedfundrate and unemploymentrate by plotting scatter plots. To demonstrate this, we reduce the transparency of the dot to make sure we can determine overlapping to get more accurate perceptions of the range of songs' danceability and valence. We found out that regardless of the fed fund rate and unemployment rate, we have songs with a full spectrum of danceability and valence. This might justify why there is a weak correlation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot for fedfundrate and danceability. Danceability seems to generally shift up (become higher) when interest rate increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dance_fedfund_graph= sns.scatterplot(cleandata, x=\"fedfundrate\",\\\n",
    "                                     y=\"danceability\", alpha=0.6)\n",
    "plt.title('Scatterplot for Federal Fund Rate and Danceability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot for fedfundrate and valence. There appears to be a wide range of valence across all fedfundrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_fedfund_graph= sns.scatterplot(cleandata, \\\n",
    "                    x=\"fedfundrate\", y=\"valence\", alpha=0.6)\n",
    "plt.title('Scatterplot for Federal Fund Rate and Valence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot for unemployment rate and valence. There appears to be a wide range of danceability across all unemployment rate. However, the range of danceability seems to be slightly smaller when unemployment rate increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dance_un_graph= sns.scatterplot(cleandata, x=\"UnemploymentRate\", \\\n",
    "                                y=\"danceability\", alpha=0.6)\n",
    "plt.title('Scatterplot for Unemployment Rate and Danceability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot for unemployment rate and valence. There appears to be a wide range of valence across all unemployment rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_un_graph= sns.scatterplot(cleandata, x=\"UnemploymentRate\", \\\n",
    "                                  y=\"valence\", alpha=0.6)\n",
    "plt.title('Scatterplot for Unemployment Rate and Valence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Limitation\n",
    "\n",
    "From our journey of exploring the relationship between music sentiment and key economic variables, we came to the realization that *the navigation of the relationship we aim to discover in simplistic terms requires a further extension of qualitative research*, **especially in terms of addressing which data sources we ought to reconsider and how we address the vast range of music sentiments, and in terms of addressing the complex musical and economic natures**. \n",
    "\n",
    "To address this problem, we primarily used the Spotify (to get the quantitative values for music sentiment) dataset. Yet, it might be true that the **popularity column of Spotify commits recency bias. In other words, songs that were recently published are significantly more popular than the older ones**. Such matter of wrongful bias could offer explanation in why we are yet to find tracks that is evenly distributed across the years, and our methodologies must be improved to better accurately represent the most representative songs across the decade. Therefore, further exploration of methods to attain better--or more evenly distributed--data must be considered. For example, in the further phases of our analysis, we could use *Billboard charts*, the most well-known American music charting organization, across the years *and link it to the Spotify data* to normalize our distribution across time, and also have a better representation in what the American public listened to at their respective times.\n",
    "\n",
    "Even if we do solve the inequalities of the number of tracks across the years, we must **question the complex nature of music**. Indeed, the preference of vibes are from human endeavor, thus the possibility of those realms being connected by human endeavor and psychological elements can be held. Yet, the factor of *time* makes our relationship with music so excitingly complex. You ask a man in post-World War II--a pompous time in America--what music he listens to, and he'll answer *cool jazz* (a music with solemn vibes); you ask another man of the late 2000s--a desolate time amidst the sub-prime mortgage crisis--what music he plays on repeat, and he'll answer *electronic dance music*. Hence, the variability of musical preference is so severe--not only across time, but across individuals--that pinpointing this into a limited number of songs insults the rich nature of human complexity with music.\n",
    "\n",
    "- Data is limited to yearly basis due to the lack of monthly data available. (EXPAND ON THIS)\n",
    "\n",
    "The **economic structure can not be watered down into simplicity either**. Economics, even at the micro scale, is yet to offer a simple equation albeit the centuries of studies by the erudite. The elements of psychology, rational thinking, complex mathematics makes it even harder for one to fully comprehend the damned nature of individual interactions between one party with another. Now, consider the national scale, where many of the key economic indicators we've put in our analysis is a result of millions of interactions. Then, consider governments and federal institutions with different political leanings, of which might enact contrasting monetary and/or fiscal policies even with similar economic conditions. **The multitude of parties and factors involved that produce our key economic indicators explain why music sentiments might not directly show the a strong corelation with one of the economical index we used in our analysis.**\n",
    "\n",
    "Another limitation we have is that because **the popular songs that we have for each year is uneven**, we decided to only include popular songs from 2000-2015 where the number of songs is relatively even. This step in data cleaning not only reduce the number of data entries we have but also restrict our datasets to not have recent popular songs. \n",
    "\n",
    "- Model cannot be addressed with models that we have learned in class. Prior research has been using more complex models to determine the relationship between economic data and music sentiments.\n",
    "\n",
    "**Therefore, complexities in addressing the data, addressing sentiment, and addressing economy are reasons why we believe that further discussion and research is needed.** Given the complexities of these realms, the over-simplification to a couple of numbers \"algorithmically\" produced by Spotify and a couple of economic statistics can explain the reason why we are yet to find a strong relationship. Thankfully, by adding more variables or implementing better methods of standardization, we are able to at least progress in our question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- Given that we are not able to use models other than regression models, should we try to limit our scope of research so that we could produce a better model for our project?\n",
    "    - For example, would examining the impact of COVID to key economical indicators and music sentiment help us make a better model?\n",
    "    - Yet, if the aim of regression models is to predict, how can fit in the predictive part of our project if we constrict our model with a certain time period?\n",
    "- What would alternative methods to constrict our data other than time?\n",
    "    - Could genre be one? Tempo? or constricting the range of interest rates? unemployment rates?\n",
    "- In contrast, can we expand/modify our research question so that we could look at different realms (other than music and economics)?\n",
    "    - For example, can we modify our question so that we look at statistics such as population rates or prison rates and relate that to music sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "1. Plot 2 y variables in a graph: https://stackoverflow.com/questions/55654500/seaborn-plot-with-second-y-axis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
